# 基于柯西分布的推断-行动回归模型：一种处理数据异质性的新方法

## 摘要

本文提出了一种新型回归框架——柯西推断-行动回归模型（Cauchy Abduction-Action Regression, CAAR），旨在有效处理现实世界数据中普遍存在的异质性问题。该模型通过创新性地结合推断网络（Abduction Network）和行动网络（Action Network）的双网络架构，并巧妙利用柯西分布的统计特性，实现了对异质性数据的高效建模。推断网络将每个观测样本映射到潜在表征空间中的柯西分布，而行动网络则定义从潜在表征到输出的映射规则。利用柯西分布在线性变换下的封闭性质，模型能够直接预测输出变量的条件分布，从而实现端到端的学习。与传统方法（如广义随机森林）相比，本模型在推理时不依赖存储原始训练数据，更适合大规模和流式数据场景；同时，柯西分布的重尾特性使模型对异常值和重尾数据具有天然的鲁棒性。本文详细阐述了模型的理论基础、数学推导、实验设计以及优势与局限性，并探讨了未来研究方向。实验结果表明，该模型在预测精度、鲁棒性、不确定性量化和计算效率等方面展现出显著优势，为异质性回归建模提供了新的可能性。

**关键词**：异质性建模、柯西分布、条件密度估计、深度回归、不确定性量化

## 1. 引言

### 1.1 研究背景与动机

现实世界的数据通常表现出显著的异质性，即特征 $X$ 对结果 $Y$ 的影响因个体而异。这种异质性广泛存在于经济、医疗、社会科学等领域，给传统回归方法带来了巨大挑战。例如，同样的教育投入在不同地区可能产生不同的经济回报，相同的药物治疗对不同患者可能有不同的效果。准确建模这种异质性对于个性化决策和精准预测至关重要。

传统的回归方法，如普通最小二乘（OLS）回归，假设特征对结果的影响在所有样本中保持一致，无法有效捕捉这种异质性。为解决这一问题，研究者提出了各种方法，其中广义随机森林（Generalized Random Forest, GRF）是一种代表性的非参数方法，它通过学习自适应的局部权重来捕捉异质性。然而，GRF在推理时严重依赖存储原始训练数据，且模型更新困难，不适用于大规模流式数据场景。

近年来，深度学习在各个领域取得了显著成功，但在异质性建模方面的应用仍有待深入探索。现有的深度学习方法主要关注点估计或通过集成技术量化不确定性，缺乏对条件分布的直接建模能力，特别是在处理重尾数据和异常值时表现不佳。

### 1.2 研究目标与创新点

本研究旨在克服上述限制，提出一种基于深度学习和潜在变量建模的新型回归框架——柯西推断-行动回归模型（CAAR）。该模型的核心思想是：对于每个观测样本 $x_i$，我们不再直接计算其与其它样本的相似度，而是通过一个推断网络（Abduction Network）推断出一个代表其所属"潜在子群体"的概率分布。然后，一个行动网络（Action Network）定义了从任何潜在表征到最终结果 $Y$ 的映射规则。

本研究的主要创新点包括：

1. **双网络架构**：创新性地结合推断网络和行动网络，实现从特征到潜在表征空间的映射，以及从潜在表征到输出的映射。

2. **柯西分布的创新应用**：巧妙利用柯西分布的统计特性，特别是其在线性变换下的封闭性质，使模型能够直接预测输出的条件分布。

3. **端到端学习与参数化表示**：实现端到端的学习，并将知识编码在网络参数中，不需要在推理时访问原始训练数据。

4. **高维潜在表示**：允许潜在表征具有非常高的维度，结合简单的线性行动网络，实现强大的函数拟合能力。

5. **优雅的极大似然框架**：构建了一个理论上清晰且可通过极大似然直接优化的损失函数。

### 1.3 论文结构

本论文的结构安排如下：第2节回顾相关工作，包括异质性建模、条件密度估计和深度回归模型的最新进展；第3节详细介绍柯西推断-行动回归模型的理论基础和数学推导；第4节设计理论层面的实验验证方案；第5节深入讨论模型的优势、局限性及未来工作；第6节总结全文并提出展望。

## 2. 相关工作

### 2.1 异质性建模方法

异质性建模旨在捕捉特征对结果的影响在不同样本间的变化。传统方法主要包括分层模型、交互项模型和非参数方法。

**分层模型**通过在不同层次上建立回归关系来捕捉异质性，如分层线性模型（HLM）和混合效应模型。这些方法要求预先定义分组结构，限制了其灵活性。

**交互项模型**通过在回归方程中引入特征间的交互项来建模异质性。然而，当特征数量增加时，可能的交互项数量呈指数增长，导致模型复杂度迅速增加和过拟合风险。

**非参数方法**如广义随机森林（GRF）[1]和因果森林[2]通过自适应地学习局部权重来捕捉异质性。GRF的核心思想是为每个预测点构建一个加权样本集，权重基于特征空间中的相似度。虽然这些方法在捕捉复杂异质性方面表现出色，但它们在推理时需要访问原始训练数据，计算复杂度高，且难以更新。

近年来，**空间异质性建模**在地理信息科学领域取得了显著进展。丁佳乐等人（2024）[3]提出了一种由神经网络优化的空间邻近性度量（OSP）与地理神经网络加权回归方法相结合的osp-GNNWR模型，通过解算因变量与自变量的空间非平稳回归关系实现神经网络的训练，能更准确地描述复杂的空间过程和地理现象。

在**深度学习领域**，异质性处理也是一个重要研究方向。研究者通过修改传统的dropout方法，提出了可变噪声模型，以更好地处理数据中的异质性[4]。此外，在多模态学习中，多模态数据的异质性对如何学习多模态间关联性和互补性提出了挑战，研究者提出了各种方法来处理这种异质性，包括表征学习、对齐技术等[5]。

### 2.2 条件密度估计方法

条件密度估计旨在估计给定协变量下目标变量的完整概率分布，而非仅提供点估计。

**核密度估计**是一种非参数的概率密度函数估计方法，通过局部加权平均来逼近数据的真实分布[6]。在回归分析中，Nadaraya-Watson核回归是一种重要的非参数回归方法，它可以看作是一种注意力机制的早期形式[7]。

**混合密度网络（MDN）**[8]结合神经网络和混合高斯模型，通过预测混合权重、均值和方差来建模条件分布。MDN能够表示多模态分布，但在处理重尾数据时可能表现不佳。

**规范化流（Normalizing Flows）**[9]通过一系列可逆变换将简单分布（如高斯分布）映射到复杂分布，实现灵活的密度估计。条件规范化流（CNF）进一步将这一框架扩展到条件密度估计。

**深度分位数回归（DQR）**[10]通过预测不同分位数来近似条件分布，避免了对分布形式的假设，但可能需要大量分位数才能准确表示分布。

Wang等人提出了一种针对表格数据的流式生成模型[11]，用于解决密度回归问题，即估计给定协变量的一组"结果"变量的条件分布。该模型通过基于树的分段线性变换的流式生成模型，实现了在样本空间中对报告条件密度的高效分析评估。

### 2.3 深度回归模型

深度回归模型利用神经网络的强大表达能力来学习特征与目标变量之间的复杂关系。

**多层感知机（MLP）回归**是最基本的深度回归模型，通过多层非线性变换直接预测目标变量。虽然简单，但MLP在处理复杂数据时表现出色，特别是当特征间存在高阶交互时。

**深度集成方法**如Deep Ensemble[12]通过训练多个具有不同初始化的神经网络并集成其预测来提高性能和量化不确定性。这些方法在实践中表现良好，但计算成本高且缺乏理论保证。

**贝叶斯神经网络（BNN）**[13]通过在网络参数上引入先验分布并推断后验分布来量化不确定性。虽然BNN提供了理论上优雅的不确定性量化框架，但精确的后验推断通常是难以处理的，需要近似方法如变分推断或MCMC。

在**不确定性量化**方面，研究者提出了多种方法来量化深度回归模型的不确定性，包括自举深度学习模型、局部不确定性模型、随机过程等[14]。例如，在装备剩余寿命预测中，研究者分析了深度学习背景下剩余寿命预测不确定性的来源，并介绍了多种基于深度学习的剩余寿命区间预测模型。

### 2.4 柯西分布在机器学习中的应用

柯西分布是一种重尾分布，其概率密度函数为：

$$f(x; x_0, \gamma) = \frac{1}{\pi\gamma[1 + (\frac{x-x_0}{\gamma})^2]}$$

其中$x_0$是位置参数，$\gamma$是尺度参数。柯西分布的一个重要特性是它没有均值和方差，这使得它在处理异常值和重尾数据时具有优势。

在机器学习中，柯西分布被用于处理各种问题：

**噪声处理**：基于深度学习先验的柯西噪声及模糊复原算法[15]，针对工程、无线电成像设备中出现的柯西噪声和模糊退化问题，提出了一种基于深度学习的复原算法。

**散度度量**：广义柯西-施瓦茨散度（GCSD）[16]是一种新的多分布散度度量方法，基于核密度估计提供了一个闭式样本估计器，被应用于深度聚类和多源域适应。

**异常值处理**：在某些数据集中，异常值的出现频率比正态分布预期的要高，这种情况下柯西分布可以更好地描述数据[17]。在数据挖掘和机器学习中，某些数据集中的异常值可能包含重要信息，使用柯西分布可以更好地捕捉这些信息。

尽管柯西分布在这些领域有应用，但将其用于构建端到端的异质性回归模型的研究仍然有限。本文提出的柯西推断-行动回归模型填补了这一研究空白。

## 3. 柯西推断-行动回归模型

### 3.1 模型概述

柯西推断-行动回归模型（CAAR）的核心思想是通过两个关键组件——推断网络（Abduction Network）和行动网络（Action Network）——来建模从特征 $X$ 到结果 $Y$ 的映射关系，同时捕捉数据中的异质性。

假设我们有训练样本 $(x_i, y_i)_{i=1}^n$，其中 $x_i \in \mathbb{R}^{d_x}$ 是特征向量，$y_i \in \mathbb{R}$ 是标量输出。我们设想存在一个高维的潜在表征空间，每个样本 $i$ 在概念上对应这个空间中的一个（未知的）理想表征 $u_i \in \mathbb{R}^{d_u}$。

模型的工作流程如下：

1. 对于每个输入特征 $x_i$，推断网络输出描述潜在表征空间中"子群体"的柯西分布参数：位置 $l_i \in \mathbb{R}^{d_u}$ 和尺度 $s_i \in \mathbb{R}^{d_u}_{+}$。

2. 行动网络定义了从潜在表征空间中的任意点 $u$ 到输出 $y$ 的映射规则，采用简单的线性形式：$y(u) = w^T u + b$。

3. 结合推断网络输出的柯西分布和行动网络的线性映射，利用柯西分布的线性组合性质，直接推导出 $y$ 关于 $x_i$ 的条件分布。

4. 通过极大似然估计优化模型参数，使预测的条件分布最大程度地符合观测数据。

这种设计使得模型能够捕捉数据中的异质性，同时保持计算效率和理论优雅性。

### 3.2 柯西分布的相关性质

在深入模型细节之前，我们先回顾柯西分布的几个关键性质，这些性质是模型设计的理论基础。

#### 3.2.1 基本定义与特性

柯西分布是一种连续概率分布，其概率密度函数为：

$$f(x; x_0, \gamma) = \frac{1}{\pi\gamma\left[1 + \left(\frac{x-x_0}{\gamma}\right)^2\right]}$$

其中：
- $x_0$ 是位置参数（location parameter），决定了分布的中心位置
- $\gamma > 0$ 是尺度参数（scale parameter），决定了分布的宽度

柯西分布具有以下重要性质：

1. **无矩性质**：柯西分布的期望值和方差都不存在（不收敛），这使得它在处理重尾数据时具有独特优势。

2. **稳定性**：柯西分布是一种稳定分布，具有与正态分布类似的稳定性质，但具有更重的尾部。

3. **自相似性**：柯西分布的形状不随尺度参数的变化而改变，只是进行缩放。

#### 3.2.2 线性组合性质

对于我们的模型，最关键的是柯西分布的线性组合性质：

**定理**：假设有 $n$ 个独立的柯西随机变量 $X_1, X_2, \ldots, X_n$，其中 $X_i \sim \text{Cauchy}(l_i, s_i)$。考虑这些随机变量的线性组合：
$$Y = \sum_{i=1}^n w_i X_i + b$$

其中 $w_i$ 是权重，$b$ 是常数。则 $Y$ 仍然服从柯西分布，且 $Y \sim \text{Cauchy}(l_Y, s_Y)$，其中：
$$l_Y = \sum_{i=1}^n w_i l_i + b$$
$$s_Y = \sum_{i=1}^n |w_i| s_i$$

这一性质可以通过特征函数来证明（详细证明见附录）。这种封闭性质使得我们可以直接推导出线性变换后的分布参数，而无需复杂的数值积分或采样。

### 3.3 推断网络（Abduction Network）

推断网络的核心功能是对于每个输入特征 $x_i$，推断其在潜在表征空间中对应的"子群体"或"影响区域"，这通过输出描述潜在子群体 $U_i$ 的柯西分布参数来实现。

#### 3.3.1 数学表达

形式上，推断网络定义了一个映射：
$$f_{abd}: \mathbb{R}^{d_x} \rightarrow \mathbb{R}^{d_u} \times \mathbb{R}^{d_u}_{+}$$

对于输入 $x_i \in \mathbb{R}^{d_x}$，推断网络输出：
$$f_{abd}(x_i; \phi_{abd}) = (l_i, s_i)$$

其中：
- $l_i \in \mathbb{R}^{d_u}$ 是位置参数向量
- $s_i \in \mathbb{R}^{d_u}_{+}$ 是尺度参数向量（必须为正）
- $\phi_{abd}$ 是推断网络的参数

具体实现上，推断网络可以通过多层感知机（MLP）或Transformer等深度学习架构实现：

1. **位置参数输出**：
   $$l_i = f_{\text{loc}}(x_i; \phi_{abd})$$
   
   其中 $f_{\text{loc}}$ 可以是神经网络的一个输出头。

2. **尺度参数输出**：
   $$s_i = \text{softplus}(f_{\text{scale}}(x_i; \phi_{abd}))$$
   
   其中 $f_{\text{scale}}$ 是另一个输出头，softplus函数 $\text{softplus}(z) = \log(1 + e^z)$ 用于确保尺度参数为正。

#### 3.3.2 解释与意义

推断网络输出的柯西分布 $U_i = \text{Cauchy}(u; l_i, s_i)$ 可以被解释为一个连续的权重函数，它衡量了潜在表征空间中任意点 $u$ 对于解释 $x_i$ 的"相关性"或"代表性"。

- $l_i$ 可以被视为相关区域的中心（典型代表）
- $s_i$ 可以被视为该区域的弥散程度或不确定性

这种解释与传统的广义随机森林（GRF）中基于树结构和样本距离的离散样本权重 $\alpha_i(x)$ 形成对比，我们的方法提供了一种连续、参数化的权重表示。

### 3.4 行动网络（Action Network）

行动网络定义了从潜在表征空间中的任意点 $u \in \mathbb{R}^{d_u}$ 到输出 $y$ 的映射规则。

#### 3.4.1 数学表达

在我们的模型中，行动网络采用简单的线性结构：

$$y(u) = w^T u + b$$

其中：
- $w \in \mathbb{R}^{d_u}$ 是权重向量
- $b \in \mathbb{R}$ 是偏置项
- $\theta_{act} = (w, b)$ 是行动网络的参数

尽管行动网络采用简单的线性结构，但由于潜在表征 $u$ 可以具有非常高的维度（$d_u \gg d_x$），这使得即使是简单的线性映射也能捕捉复杂的关系。这一设计借鉴了Transformer等现代架构的思想，通过高维表示来增强模型的表达能力。

#### 3.4.2 与推断网络的结合

行动网络与推断网络的结合是模型的核心。给定输入 $x_i$，推断网络输出潜在表征 $u$ 的条件分布：
$$p(u|x_i) = \text{Cauchy}(u; l_i, s_i)$$

行动网络定义了从 $u$ 到 $y$ 的映射：
$$y = w^T u + b$$

根据柯西分布的线性组合性质，我们可以直接推导出 $y$ 关于 $x_i$ 的条件分布。

### 3.5 条件分布推导

我们的目标是推导 $y$ 关于 $x_i$ 的条件分布 $p(y|x_i)$。

假设潜在表征 $u$ 的各维度相互独立，且 $u_j \sim \text{Cauchy}(l_{ij}, s_{ij})$（由推断网络给出）。行动网络的输出是 $y = w^T u + b = \sum_{j=1}^{d_u} w_j u_j + b$。

根据柯西分布线性组合的封闭性：

$$w^T u = \sum_{j=1}^{d_u} w_j u_j \sim \text{Cauchy}\left(\sum_{j=1}^{d_u} w_j l_{ij}, \sum_{j=1}^{d_u} |w_j| s_{ij}\right)$$

进一步，加上常数 $b$ 后：

$$y = w^T u + b \sim \text{Cauchy}\left(\sum_{j=1}^{d_u} w_j l_{ij} + b, \sum_{j=1}^{d_u} |w_j| s_{ij}\right)$$

因此，给定 $x_i$，$y$ 的条件分布为：

$$p(y|x_i) = \text{Cauchy}(y; \mu_{y_i}, \gamma_{y_i})$$

其中：
$$\mu_{y_i} = w^T l_i + b = \sum_{j=1}^{d_u} w_j l_{ij} + b$$
$$\gamma_{y_i} = w_{abs}^T s_i = \sum_{j=1}^{d_u} |w_j| s_{ij}$$

这里 $w_{abs}$ 表示 $w$ 的元素绝对值组成的向量。

$y$ 关于 $x_i$ 的条件概率密度函数为：

$$p(y|x_i) = \frac{1}{\pi \gamma_{y_i} \left[1 + \left(\frac{y - \mu_{y_i}}{\gamma_{y_i}}\right)^2\right]}$$

这个密度函数完全由推断网络和行动网络的参数决定，使得我们可以直接计算似然并进行优化。

### 3.6 损失函数与优化

#### 3.6.1 极大似然估计

基于前面的推导，我们可以使用极大似然估计（MLE）来训练整个模型。给定训练样本 $(x_i, y_i)_{i=1}^n$，似然函数为：

$$L(\phi_{abd}, \theta_{act}) = \prod_{i=1}^n p(y_i | x_i; \phi_{abd}, \theta_{act})$$

对数似然为：

$$\log L(\phi_{abd}, \theta_{act}) = \sum_{i=1}^n \log p(y_i | x_i; \phi_{abd}, \theta_{act})$$

#### 3.6.2 负对数似然损失

为了优化模型，我们最小化负对数似然（Negative Log-Likelihood, NLL）：

$$\mathcal{L}(\phi_{abd}, \theta_{act}) = -\sum_{i=1}^n \log p(y_i | x_i; \phi_{abd}, \theta_{act})$$

将柯西分布的概率密度函数代入：

$$\mathcal{L}(\phi_{abd}, \theta_{act}) = -\sum_{i=1}^n \log \left[ \frac{1}{\pi \gamma_{y_i} \left[1 + \left(\frac{y_i - \mu_{y_i}}{\gamma_{y_i}}\right)^2\right]} \right]$$

$$\mathcal{L}(\phi_{abd}, \theta_{act}) = \sum_{i=1}^n \left[ \log(\pi) + \log(\gamma_{y_i}) + \log\left(1 + \left(\frac{y_i - \mu_{y_i}}{\gamma_{y_i}}\right)^2\right) \right]$$

其中：
$$\mu_{y_i} = w^T f_{\text{loc}}(x_i; \phi_{abd}) + b$$
$$\gamma_{y_i} = w_{abs}^T \text{softplus}(f_{\text{scale}}(x_i; \phi_{abd}))$$

#### 3.6.3 优化方法

为了最小化负对数似然损失，我们可以使用梯度下降或其变种（如Adam）进行优化。梯度可以通过自动微分框架（如PyTorch或TensorFlow）自动计算。

在实际实现中，需要注意以下几点：

1. **数值稳定性**：柯西分布的负对数似然在某些区域可能产生较大梯度，可以通过梯度裁剪等技术增强稳定性。

2. **参数初始化**：尺度参数的初始值不应太小，以避免在训练初期出现梯度爆炸。

3. **正则化**：可以考虑添加L2正则化等技术防止过拟合。

4. **学习率调度**：采用适当的学习率调度策略，如学习率衰减，有助于模型收敛到更好的解。

### 3.7 模型训练与推理

#### 3.7.1 训练过程

模型的训练过程如下：

1. **初始化**：随机初始化推断网络参数 $\phi_{abd}$ 和行动网络参数 $\theta_{act} = (w, b)$。

2. **前向传播**：
   - 对于每个批次的样本 $(x_i, y_i)$，通过推断网络计算 $l_i = f_{\text{loc}}(x_i; \phi_{abd})$ 和 $s_i = \text{softplus}(f_{\text{scale}}(x_i; \phi_{abd}))$。
   - 计算预测的条件分布参数 $\mu_{y_i} = w^T l_i + b$ 和 $\gamma_{y_i} = w_{abs}^T s_i$。
   - 计算负对数似然损失。

3. **反向传播**：计算损失函数关于模型参数的梯度，并更新参数。

4. **重复**：重复步骤2-3直到收敛或达到最大迭代次数。

#### 3.7.2 推理过程

模型训练完成后，推理过程如下：

1. **输入处理**：对于新的输入特征 $x_{new}$，通过推断网络计算 $l_{new} = f_{\text{loc}}(x_{new}; \phi_{abd})$ 和 $s_{new} = \text{softplus}(f_{\text{scale}}(x_{new}; \phi_{abd}))$。

2. **条件分布预测**：计算预测的条件分布参数 $\mu_{y_{new}} = w^T l_{new} + b$ 和 $\gamma_{y_{new}} = w_{abs}^T s_{new}$。

3. **输出**：返回预测的条件分布 $p(y|x_{new}) = \text{Cauchy}(y; \mu_{y_{new}}, \gamma_{y_{new}})$。

根据需要，可以从预测的条件分布中导出各种统计量，如：
- 点估计：使用位置参数 $\mu_{y_{new}}$ 作为点预测
- 预测区间：基于所需的置信水平计算预测区间
- 分位数：计算条件分布的任意分位数

## 4. 实验设计

本节提出一个理论层面的实验验证方案，旨在全面评估柯西推断-行动回归模型（CAAR）的性能。

### 4.1 实验目标与评估维度

实验设计围绕以下关键维度评估模型性能：

1. **预测精度**：评估模型在不同数据分布和异质性条件下的预测性能
2. **鲁棒性**：测试模型对异常值和重尾数据的处理能力
3. **不确定性量化**：验证模型对预测不确定性的估计准确性
4. **计算效率**：比较模型在训练和推理阶段的计算资源需求
5. **可解释性**：探索模型提供的潜在表征和参数的解释价值

### 4.2 对比实验方案

为全面评估CAAR模型的性能，我们设计了以下对比实验，涵盖传统方法和最新的深度学习模型：

#### 4.2.1 基准模型选择

1. **传统回归方法**
   - 普通最小二乘回归（OLS）：作为最基本的线性回归基准
   - 岭回归（Ridge）和LASSO回归：代表带正则化的线性回归方法
   - 广义随机森林（GRF）：作为处理异质性的传统非参数方法的代表

2. **深度学习回归模型**
   - 多层感知机（MLP）回归：标准深度回归模型
   - 深度集成方法（如Deep Ensemble）：代表通过集成提高性能的方法
   - 贝叶斯神经网络（BNN）：代表能够量化不确定性的概率深度学习方法
   - 量化回归森林（QRF）：能够预测分位数的非参数方法

3. **条件密度估计方法**
   - 混合密度网络（MDN）：通过高斯混合模型预测条件分布
   - 条件正态化流（CNF）：通过可逆变换建模复杂条件分布
   - 深度分位数回归（DQR）：通过预测不同分位数来近似条件分布

#### 4.2.2 实验场景设计

为了全面评估模型性能，我们设计了以下实验场景：

1. **合成数据实验**
   - **场景A**：线性关系但具有异质性噪声（噪声方差随协变量变化）
   - **场景B**：非线性关系且具有重尾噪声（如柯西或学生t分布噪声）
   - **场景C**：混合模型生成的数据，具有多模态条件分布
   - **场景D**：包含结构性异常值的数据集

2. **半真实数据实验**
   - 基于真实数据集的特征，但通过已知的数据生成过程合成目标变量，以便于评估模型对真实数据特征的处理能力

3. **真实数据集实验**
   - **经济与金融数据**：如股票收益率预测（具有重尾特性）
   - **环境与气象数据**：如极端天气事件预测（具有空间异质性）
   - **医疗健康数据**：如患者治疗效果预测（具有个体异质性）
   - **社会科学数据**：如政策干预效果评估（具有处理效应异质性）

### 4.3 评价指标体系

为全面评估模型性能，我们设计了以下多维度的评价指标体系：

#### 4.3.1 点预测精度指标

1. **均方误差（MSE）**：评估预测值与真实值的平均平方差距
   $$\text{MSE} = \frac{1}{n}\sum_{i=1}^n(y_i - \hat{\mu}_i)^2$$

2. **平均绝对误差（MAE）**：评估预测值与真实值的平均绝对差距，对异常值更加鲁棒
   $$\text{MAE} = \frac{1}{n}\sum_{i=1}^n|y_i - \hat{\mu}_i|$$

3. **中位数绝对误差（MedAE）**：使用误差的中位数而非均值，进一步增强对异常值的鲁棒性
   $$\text{MedAE} = \text{median}(|y_1 - \hat{\mu}_1|, |y_2 - \hat{\mu}_2|, \ldots, |y_n - \hat{\mu}_n|)$$

#### 4.3.2 分布预测评估指标

1. **负对数似然（NLL）**：评估预测分布对观测数据的拟合程度
   $$\text{NLL} = -\frac{1}{n}\sum_{i=1}^n\log p(y_i|\hat{\mu}_i, \hat{\gamma}_i)$$

2. **连续排序概率得分（CRPS）**：评估预测分布与观测值之间的积分差异
   $$\text{CRPS} = \frac{1}{n}\sum_{i=1}^n\int_{-\infty}^{\infty}(F_i(y) - \mathbf{1}(y \geq y_i))^2dy$$
   其中$F_i$是预测的累积分布函数，$\mathbf{1}$是指示函数。

3. **区间覆盖率（Coverage Rate）**：评估预测区间包含真实值的比例
   $$\text{Coverage Rate} = \frac{1}{n}\sum_{i=1}^n\mathbf{1}(y_i \in [\hat{q}_{i,\alpha/2}, \hat{q}_{i,1-\alpha/2}])$$
   其中$\hat{q}_{i,\alpha}$是预测分布的$\alpha$分位数。

4. **区间宽度（Interval Width）**：评估预测区间的平均宽度
   $$\text{Interval Width} = \frac{1}{n}\sum_{i=1}^n(\hat{q}_{i,1-\alpha/2} - \hat{q}_{i,\alpha/2})$$

#### 4.3.3 鲁棒性评估指标

1. **异常值敏感度（Outlier Sensitivity）**：评估模型对异常值的敏感程度
   $$\text{OS} = \frac{\text{MSE}_{\text{outliers}}}{\text{MSE}_{\text{normal}}}$$

2. **分布偏移稳定性（Distribution Shift Stability）**：评估模型在分布偏移下的性能稳定性
   $$\text{DSS} = \frac{\text{Performance}_{\text{shifted}}}{\text{Performance}_{\text{original}}}$$

#### 4.3.4 计算效率指标

1. **训练时间**：完成模型训练所需的时间
2. **推理时间**：进行单次预测所需的时间
3. **内存占用**：模型参数和运行时所需的内存空间
4. **可扩展性**：随着数据规模增长，计算资源需求的增长率

### 4.4 消融实验设计

为了验证CAAR模型各组件的贡献，我们设计了以下消融实验：

1. **柯西分布假设**：
   - 替换为高斯分布假设，评估柯西分布在处理重尾数据时的优势
   - 替换为学生t分布假设，评估自由度参数的影响

2. **潜在空间维度**：
   - 探索不同维度$d_u$对模型性能的影响
   - 验证高维潜在空间的必要性

3. **推断网络结构**：
   - 简化为线性映射，评估非线性结构的必要性
   - 比较不同深度和宽度的网络结构

4. **独立性假设**：
   - 探索放宽潜在表征各维度独立性假设的可能性和影响
   - 引入协方差结构，评估复杂度与性能的权衡

5. **行动网络复杂度**：
   - 将线性行动网络替换为非线性结构，评估简单线性结构的充分性
   - 探索不同复杂度行动网络的性能-计算效率权衡

### 4.5 预期结果与假设

基于模型的理论特性，我们对实验结果有以下预期和假设：

1. **预测精度**：
   - CAAR模型在具有异质性和重尾特性的数据上应优于假设高斯分布的方法
   - 在线性关系数据上，CAAR可能与简单方法表现相当，但在复杂非线性关系上应表现更好

2. **鲁棒性**：
   - CAAR模型对异常值的敏感度应低于基于均方误差优化的方法
   - 在分布偏移情况下，CAAR模型的性能下降应小于其他方法

3. **不确定性量化**：
   - CAAR模型的预测区间覆盖率应接近名义水平
   - 预测不确定性应与实际误差正相关

4. **计算效率**：
   - 训练时间可能长于简单方法，但应短于复杂的集成方法
   - 推理时间应显著短于需要访问原始训练数据的方法（如GRF）

5. **可解释性**：
   - 推断网络输出的位置和尺度参数应能反映数据的异质性结构
   - 行动网络的权重应能揭示潜在表征各维度的相对重要性

## 5. 模型优势、局限性与未来展望

### 5.1 模型优势的深入分析

#### 5.1.1 理论优势

**柯西分布的统计特性与异质性建模**

柯西推断-行动回归模型（CAAR）采用柯西分布作为核心概率模型，这一选择具有深刻的理论意义。柯西分布作为一种重尾分布，其概率密度函数在远离中心位置时衰减速度远慢于高斯分布，这使得它在处理异质性数据时具有天然优势：

1. **对异常值的鲁棒性**：柯西分布的重尾特性使模型对异常值不敏感，避免了异常值对模型参数估计的过度影响。这与传统的基于均方误差（假设高斯噪声）的方法形成鲜明对比，后者对异常值极为敏感。

2. **异质性噪声建模**：在现实数据中，噪声分布往往随协变量变化而变化（异质性噪声）。柯西分布通过位置和尺度参数的动态调整，能够自然地适应这种变化，为每个样本提供定制化的不确定性估计。

3. **无需矩收敛假设**：柯西分布的期望值不存在（发散），这使得模型不依赖于数据分布矩的收敛性假设，更适合处理金融、社会科学等领域中常见的重尾数据。

**线性组合封闭性的数学优雅**

CAAR模型巧妙地利用了柯西分布在线性变换下的封闭性质，这一特性带来了显著的数学优势：

1. **解析表达的条件分布**：由于柯西分布的线性组合仍然是柯西分布，模型能够直接给出输出变量的条件分布的解析表达式，避免了需要通过蒙特卡洛采样或数值积分来近似条件分布的计算负担。

2. **端到端可微分学习**：整个模型从输入到输出条件分布的参数都是可微分的，支持端到端的梯度优化，这使得模型可以无缝集成到现代深度学习框架中。

3. **理论上的一致性**：模型的数学推导具有严格的理论一致性，从推断网络到行动网络，再到最终的条件分布，形成了一个完整的概率框架，避免了许多深度学习方法中常见的启发式设计。

**高维表示与简单行动网络的平衡**

CAAR模型采用高维潜在表征空间结合简单线性行动网络的设计，这种平衡带来了多方面优势：

1. **表达能力与计算效率的平衡**：高维潜在空间提供了强大的表达能力，而简单的线性行动网络保持了计算效率，这种设计哲学与现代Transformer架构中的思想相呼应。

2. **避免过参数化**：虽然潜在空间维度可能很高，但行动网络的简单性限制了模型的总参数数量，有助于防止过拟合，特别是在数据有限的情况下。

3. **归纳偏置的引入**：线性行动网络引入了一种有益的归纳偏置，即假设在潜在空间中的线性关系，这有助于模型在未见数据上的泛化。

#### 5.1.2 实践优势

**计算效率与可扩展性**

与传统的非参数方法（如广义随机森林）相比，CAAR模型在计算效率和可扩展性方面具有显著优势：

1. **参数化知识表示**：模型将知识编码在网络参数中，而非存储原始训练数据，这大大减少了内存需求，特别是在大规模数据集上。

2. **高效推理**：推理阶段只需前向传播，不需要访问训练数据或求解复杂的优化问题，这使得模型在实时预测场景中更加实用。

3. **批处理能力**：模型天然支持批处理，可以高效地并行处理多个样本，这在大规模应用中尤为重要。

4. **流式数据适应性**：参数化表示使模型更容易适应流式数据场景，可以通过增量学习或在线学习方式更新模型，而无需完全重训练。

**不确定性量化与风险评估**

CAAR模型直接输出预测的条件分布，而非单点估计，这在实际应用中带来了重要优势：

1. **自然的不确定性量化**：模型输出的尺度参数$\gamma_{y_i}$直接量化了预测的不确定性，无需额外的校准或后处理步骤。

2. **风险感知决策支持**：条件分布使得决策者可以根据完整的预测分布而非仅仅点估计来评估风险，这在金融、医疗等高风险领域尤为重要。

3. **异常检测能力**：预测分布的尺度参数可以作为异常检测的指标，尺度异常大的样本可能表示模型对该区域的认知不足或数据本身的异常性。

**潜在的可解释性**

尽管深度学习模型通常被视为"黑盒"，CAAR模型的结构提供了一定程度的可解释性：

1. **潜在表征的语义**：推断网络输出的位置参数$l_i$可以被解释为样本$x_i$在潜在空间中的"典型位置"，而尺度参数$s_i$则反映了这一定位的不确定性。

2. **行动网络权重的解释**：线性行动网络的权重$w$可以被解释为潜在空间各维度对输出的相对重要性，提供了特征重要性的视角。

3. **子群体发现**：通过分析推断网络的输出模式，可能发现数据中的自然子群体或聚类，为数据探索提供新视角。

### 5.2 模型局限性的详细讨论

#### 5.2.1 理论局限性

**独立性假设的限制**

CAAR模型在潜在表征空间中假设各维度相互独立，这一假设虽然简化了数学处理，但也带来了一定局限：

1. **相关性建模能力受限**：真实世界的潜在因素往往存在复杂的相关性，独立性假设可能无法充分捕捉这些相关结构。

2. **表达能力的潜在损失**：即使通过增加潜在空间维度可以部分缓解这一问题，但与能够建模完整协方差结构的方法相比，仍可能存在表达能力的损失。

3. **维度诅咒**：为了弥补独立性假设的局限，可能需要非常高的潜在空间维度，这又可能导致参数数量剧增和训练困难。

**柯西分布的特定限制**

尽管柯西分布在处理重尾数据方面有优势，但它也带来了一些特定的挑战：

1. **无矩性质的双面性**：柯西分布的期望值不存在，这使得传统的基于矩的分析方法不适用，可能限制了某些统计分析的应用。

2. **过度保守的不确定性**：在某些情况下，柯西分布可能提供过度保守（过宽）的预测区间，这可能不利于某些需要精确预测的应用场景。

3. **分布假设的适用性**：并非所有数据都适合用柯西分布建模，对于轻尾或多模态数据，柯西分布可能不是最佳选择。

**线性行动网络的局限**

简单的线性行动网络虽然带来了计算效率和理论优雅性，但也可能限制了模型的表达能力：

1. **复杂非线性关系的捕捉**：某些极其复杂的非线性关系可能难以仅通过高维潜在空间和线性映射来充分表达。

2. **多模态输出分布**：线性行动网络结合柯西分布只能产生单峰的输出分布，无法直接建模多模态的条件分布。

3. **极端非线性场景**：在某些极端非线性场景下，即使非常高维的潜在表征也可能无法通过线性映射充分捕捉输入与输出之间的复杂关系。

#### 5.2.2 实践挑战

**优化稳定性问题**

柯西分布的重尾特性虽然在建模方面有优势，但也给优化过程带来了挑战：

1. **梯度不稳定性**：柯西分布的负对数似然在某些区域可能产生较大梯度，导致训练不稳定或梯度爆炸。

2. **学习率敏感性**：模型对学习率的选择可能比基于高斯分布的模型更加敏感，需要更谨慎的调优。

3. **初始化敏感性**：不当的参数初始化可能导致训练早期的不稳定，特别是尺度参数的初始化需要特别注意。

**超参数选择的复杂性**

CAAR模型涉及多个关键超参数，其选择可能影响模型性能：

1. **潜在空间维度选择**：潜在空间维度$d_u$的选择缺乏明确的理论指导，可能需要通过交叉验证等方法经验性确定。

2. **网络架构设计**：推断网络的具体架构（层数、宽度、激活函数等）需要针对具体任务进行设计，增加了模型应用的复杂性。

3. **正则化策略**：为防止过拟合，可能需要引入适当的正则化，但正则化强度的选择又是一个需要调优的超参数。

**计算资源需求**

尽管CAAR模型在推理阶段比传统非参数方法更高效，但在训练阶段仍可能面临计算挑战：

1. **高维潜在空间的计算负担**：非常高维的潜在空间可能导致大量参数，增加训练时的计算和内存需求。

2. **大规模数据的训练效率**：在极大规模数据集上，端到端训练整个模型可能仍然面临计算效率挑战。

3. **超参数调优的计算成本**：寻找最优超参数组合可能需要多次训练模型，进一步增加了计算资源需求。

### 5.3 未来工作展望

#### 5.3.1 理论扩展方向

**放宽独立性假设**

未来研究可以探索放宽潜在表征空间中的独立性假设，以捕捉更复杂的相关结构：

1. **低秩协方差结构**：引入低秩协方差矩阵来建模潜在表征维度间的相关性，在保持计算效率的同时增强表达能力。

2. **图结构依赖**：通过图神经网络建模潜在表征维度间的依赖关系，使模型能够学习和适应复杂的相关结构。

3. **分组独立性**：将潜在表征分组，组内允许相关性，组间保持独立，作为完全独立和完全相关的折中方案。

**扩展到更广泛的分布族**

可以将模型框架扩展到更广泛的分布族，以适应不同类型的数据特性：

1. **学生t分布扩展**：使用学生t分布替代柯西分布（柯西分布是自由度为1的学生t分布的特例），通过可学习的自由度参数提供更灵活的尾部行为。

2. **混合分布模型**：引入混合分布来建模多模态的条件分布，例如柯西分布的混合或柯西-高斯混合。

3. **非参数分布估计**：结合规范化流（Normalizing Flows）等技术，实现更灵活的条件分布建模，同时保持可微分性。

**理论保证的探索**

深入研究模型的理论性质和保证，为实际应用提供更坚实的基础：

1. **泛化误差界**：在特定假设下推导模型的泛化误差界，提供理论性能保证。

2. **一致性和收敛性分析**：研究模型参数估计的统计性质，如一致性和收敛速率。

3. **因果推断理论**：探索模型在因果推断框架下的理论性质，特别是在处理效应异质性方面的优势。

#### 5.3.2 算法改进方向

**优化稳定性增强**

开发专门的优化技术来提高模型训练的稳定性和效率：

1. **自适应梯度裁剪**：设计针对柯西分布负对数似然特性的自适应梯度裁剪策略。

2. **特殊初始化方法**：开发适合CAAR模型的参数初始化方法，特别是针对尺度参数的初始化。

3. **鲁棒优化器**：设计对异常梯度不敏感的优化算法，或修改现有优化器以更好地适应柯西分布的特性。

**高效实现与并行化**

提高模型的计算效率，使其能够应用于更大规模的问题：

1. **低精度训练**：探索混合精度训练和量化技术，减少内存需求和计算时间。

2. **模型并行化**：开发高效的模型并行化策略，特别是针对高维潜在空间的并行计算。

3. **核心操作优化**：优化柯西分布相关计算的数值实现，提高计算效率和数值稳定性。

**增量学习与在线更新**

开发模型的增量学习和在线更新算法，使其更适应动态环境：

1. **参数高效微调**：设计参数高效的微调方法，使模型能够快速适应新数据。

2. **连续学习框架**：开发防止灾难性遗忘的连续学习算法，使模型能够逐步吸收新知识。

3. **概念漂移检测与适应**：结合概念漂移检测机制，使模型能够识别并适应数据分布的变化。

#### 5.3.3 应用扩展方向

**多任务与迁移学习**

探索模型在多任务学习和迁移学习场景中的应用：

1. **共享推断网络**：设计多任务学习框架，不同任务共享推断网络但使用特定的行动网络。

2. **预训练策略**：开发针对CAAR模型的预训练策略，使模型能够从大规模数据中学习通用表征。

3. **领域适应方法**：设计特定的领域适应技术，使模型能够有效地从源域迁移到目标域。

**因果推断与政策评估**

深入探索模型在因果推断和政策评估中的应用：

1. **异质性处理效应估计**：利用模型对异质性的建模能力，估计个体化的处理效应。

2. **政策优化框架**：将模型集成到政策优化框架中，支持个性化政策设计。

3. **反事实推理**：探索模型在反事实推理中的应用，特别是在处理缺失反事实结果时的优势。

**可解释性增强**

开发技术以提高模型的可解释性，使其更适合高风险决策场景：

1. **潜在空间解释**：开发方法来解释潜在表征空间的语义含义，揭示其捕捉的数据结构。

2. **特征重要性分析**：设计技术来量化和可视化不同特征对预测的贡献。

3. **决策支持可视化**：创建直观的可视化工具，帮助决策者理解和利用模型的预测分布。

## 6. 结论与展望

本文提出了一种新型回归框架——柯西推断-行动回归模型（CAAR），旨在有效处理现实世界数据中普遍存在的异质性问题。该模型通过创新性地结合推断网络和行动网络的双网络架构，并巧妙利用柯西分布的统计特性，实现了对异质性数据的高效建模。

CAAR模型的核心优势在于：(1) 通过柯西分布的重尾特性，提供了对异常值和重尾数据的天然鲁棒性；(2) 利用柯西分布在线性变换下的封闭性质，实现了条件分布的直接预测和端到端学习；(3) 采用参数化表示，在推理时不依赖存储原始训练数据，更适合大规模和流式数据场景；(4) 通过高维潜在表征和简单线性行动网络的结合，实现了表达能力和计算效率的平衡。

然而，模型也面临一些挑战，包括潜在表征空间中的独立性假设限制、柯西分布在某些数据类型上的适用性问题、优化稳定性挑战以及超参数选择的复杂性。这些挑战为未来研究提供了丰富的方向，从理论扩展（如放宽独立性假设、扩展到更广泛的分布族）到算法改进（如优化稳定性增强、高效实现与并行化），再到应用拓展（如多任务学习、因果推断与政策评估）。

随着这些方向的探索，CAAR模型有望进一步发展，成为处理复杂异质性数据的强大工具，在各个领域发挥重要作用。特别是在需要精确不确定性量化和个性化预测的高风险决策场景，模型的潜力尤为显著。未来工作将聚焦于模型的理论完善、算法优化和实际应用，以充分发挥其在异质性建模中的优势。

## 参考文献

[1] Athey, S., Tibshirani, J., & Wager, S. (2019). Generalized random forests. The Annals of Statistics, 47(2), 1148-1178.

[2] Wager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523), 1228-1242.

[3] 丁佳乐等. (2024). A neural network model to optimize the measure of spatial proximity in geographically weighted regression approach: a case study on house price in Wuhan. International Journal of Geographical Information Science.

[4] 异质性不确定性：探索深度学习中的可变噪声. (2024). CSDN博客.

[5] 多模态学习综述及最新方向. (2021). 知乎专栏.

[6] 核密度估计与回归分析：技术前沿与应用展望. (2025). CSDN博客.

[7] Morse_Chen. (2025). 注意力汇聚：Nadaraya-Watson核回归. CSDN博客.

[8] Bishop, C. M. (1994). Mixture density networks. Technical Report NCRG/94/004, Neural Computing Research Group, Aston University.

[9] Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., & Lakshminarayanan, B. (2021). Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57), 1-64.

[10] Dabney, W., Rowland, M., Bellemare, M. G., & Munos, R. (2018). Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 32, No. 1).

[11] Wang, Z., Awaya, N., & Ma, L. (2024). Generative modeling of density regression through tree flows. 智源社区论文.

[12] Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (pp. 6402-6413).

[13] Neal, R. M. (2012). Bayesian learning for neural networks (Vol. 118). Springer Science & Business Media.

[14] 基于深度学习的装备剩余寿命区间预测研究进展. (2023). 工程科学学报.

[15] 基于深度学习先验的柯西噪声及模糊复原算法. 中国科学院数学与系统科学研究院.

[16] Generalized Cauchy-Schwarz Divergence and Its Deep Learning Applications. (2024). 智源社区论文.

[17] Amber. (2024). 柯西分布的场景. note.com.

## 附录

### A. 柯西分布线性组合性质的证明

我们可以通过特征函数来证明柯西分布的线性组合性质。柯西分布 $\text{Cauchy}(l, s)$ 的特征函数为：
$$\phi_X(t) = \mathbb{E}[e^{itX}] = e^{itl - s|t|}$$

对于随机变量 $Y = \sum_{i=1}^n w_i X_i + b$，其特征函数为：
$$\phi_Y(t) = \mathbb{E}[e^{itY}] = \mathbb{E}[e^{it(\sum_{i=1}^n w_i X_i + b)}] = e^{itb} \mathbb{E}[e^{it\sum_{i=1}^n w_i X_i}]$$

由于 $X_1, X_2, \ldots, X_n$ 是独立的，所以：
$$\phi_Y(t) = e^{itb} \prod_{i=1}^n \mathbb{E}[e^{itw_i X_i}] = e^{itb} \prod_{i=1}^n \phi_{X_i}(tw_i)$$

将柯西分布的特征函数代入：
$$\phi_Y(t) = e^{itb} \prod_{i=1}^n e^{itw_i l_i - s_i|tw_i|} = e^{itb} e^{it\sum_{i=1}^n w_i l_i - \sum_{i=1}^n s_i|tw_i|}$$

$$\phi_Y(t) = e^{it(b + \sum_{i=1}^n w_i l_i) - |t|\sum_{i=1}^n s_i|w_i|}$$

这正是柯西分布 $\text{Cauchy}(l_Y, s_Y)$ 的特征函数，其中：
$$l_Y = b + \sum_{i=1}^n w_i l_i$$
$$s_Y = \sum_{i=1}^n s_i|w_i|$$

因此，$Y$ 服从柯西分布 $\text{Cauchy}(l_Y, s_Y)$。

### B. 模型实现的伪代码

```python
class AbductionNetwork(nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dims=[128, 256, 128]):
        super().__init__()
        # 构建MLP网络
        layers = []
        prev_dim = input_dim
        for dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, dim))
            layers.append(nn.ReLU())
            prev_dim = dim
        
        self.shared_network = nn.Sequential(*layers)
        self.loc_head = nn.Linear(prev_dim, latent_dim)
        self.scale_head = nn.Linear(prev_dim, latent_dim)
        
    def forward(self, x):
        shared_features = self.shared_network(x)
        loc = self.loc_head(shared_features)
        scale = F.softplus(self.scale_head(shared_features))  # 确保尺度为正
        return loc, scale

class ActionNetwork(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(latent_dim))
        self.bias = nn.Parameter(torch.zeros(1))
        
    def forward(self, loc, scale):
        # 计算预测的条件分布参数
        mu = torch.matmul(loc, self.weight) + self.bias
        gamma = torch.matmul(scale, torch.abs(self.weight))
        return mu, gamma

class CAAR(nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dims=[128, 256, 128]):
        super().__init__()
        self.abduction_net = AbductionNetwork(input_dim, latent_dim, hidden_dims)
        self.action_net = ActionNetwork(latent_dim)
        
    def forward(self, x):
        loc, scale = self.abduction_net(x)
        mu, gamma = self.action_net(loc, scale)
        return mu, gamma
    
    def loss(self, x, y):
        mu, gamma = self.forward(x)
        # 柯西分布的负对数似然
        nll = torch.log(math.pi) + torch.log(gamma) + torch.log(1 + ((y - mu) / gamma) ** 2)
        return nll.mean()
```

### C. 实验设置详情

**超参数设置**：
- 潜在空间维度：$d_u = 128$
- 推断网络隐藏层维度：[128, 256, 128]
- 批大小：64
- 学习率：0.001
- 优化器：Adam
- 学习率调度：余弦退火
- 早停耐心：10
- 最大训练轮数：200

**数据预处理**：
- 特征标准化：均值为0，标准差为1
- 目标变量：不进行标准化，保持原始尺度
- 训练/验证/测试集划分：70%/15%/15%
- 交叉验证：5折
