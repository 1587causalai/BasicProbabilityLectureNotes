\documentclass[UTF8]{beamer} % UTF8 option for ctex is often implicit with modern TeX systems but good to have
\usepackage{ctex} % 支持中文
\usepackage{amsmath}
\usepackage{amssymb} % For symbols like \mathbb{E} if needed, though you used E()

% --- Theme and Appearance ---
\usetheme{Madrid} % A popular, clean theme
% \usetheme{AnnArbor} % Another good option
% \usetheme{Singapore} % Yet another option
\usecolortheme{default} % Or try others like 'whale', 'orchid'

% Remove navigation symbols ( uncomment if you want them)
\setbeamertemplate{navigation symbols}{}

% --- Custom Colors for Theorem Blocks ---
% (You used 'exampletitle' and 'examplebody', let's define colors for them)
\setbeamercolor{exampletitle}{bg=blue!20!white, fg=black}
\setbeamercolor{examplebody}{bg=blue!5!white, fg=black}
% If you prefer to use standard Beamer blocks:
% \setbeamercolor{block title}{bg=blue!20!white, fg=black}
% \setbeamercolor{block body}{bg=blue!5!white, fg=black}
% \setbeamercolor{block title alerted}{bg=red!20!white, fg=black}
% \setbeamercolor{block body alerted}{bg=red!5!white, fg=black}


\title{第七讲 大数定律}
\author{龚鹤扬}
\institute{中国科学技术大学统计学博士(上海芯梯科技有限公司)} % Using \institute is standard for affiliation
\date{\today}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{目录}
    \tableofcontents
\end{frame}

\section{引言}
\begin{frame}{引言：大数定律的重要性}
    \begin{itemize}
        \item 本讲讨论概率论中一组重要的极限定理——\alert{大数定律 (Law of Large Numbers, LLN)}。
        \item 核心思想：在大量重复试验中，事件发生的\alert{频率}近似于其\alert{概率}，样本均值收敛于总体\alert{期望}。
        \item 大数定律是连接\alert{理论概率}与\alert{统计推断}的桥梁。
        \item 它为许多统计方法的合理性提供了理论基础。
    \end{itemize}
\end{frame}

\section{切比雪夫不等式}
\begin{frame}[shrink=5]{7.1 切比雪夫不等式 (Chebyshev's Inequality)}
    \framesubtitle{一个重要的概率上界}
    \begin{itemize}
        \item 切比雪夫不等式给出了随机变量偏离其期望值的概率的一个\alert{上界}。
        \item 这个界不依赖于随机变量的具体分布形式，只需要其\alert{期望}和\alert{方差}存在且有限。
    \end{itemize}
    \pause
    \begin{block}{定理 7.1 (切比雪夫不等式)}
        设随机变量 X 具有期望 E(X) = $\mu$ 和方差 D(X) = $\sigma^2$ (其中 $0 < \sigma^2 < +\infty$)。
        则对于任意正数 $\epsilon > 0$，有：
        \[ P(|X - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2} \]
        或者等价地：
        \[ P(|X - \mu| < \epsilon) \geq 1 - \frac{\sigma^2}{\epsilon^2} \]
    \end{block}
    \pause
    \textbf{意义与特点：}
    \begin{itemize}
        \item \alert{普适性}：对任何分布都成立（只要期望、方差存在）。
        \item \alert{实用性}：在分布未知时，可以提供一个粗略的估计。
        \item \alert{局限性}：通常这个界比较宽松，即估计可能不够精确。
    \end{itemize}
\end{frame}

\section{弱大数定律 (WLLN)}
\begin{frame}[shrink=5]{7.2 弱大数定律 (Weak Law of Large Numbers, WLLN)}
    \framesubtitle{样本均值的依概率收敛}
    弱大数定律 (WLLN) 表明，当试验次数 $n$ 足够大时，样本均值 \alert{依概率收敛} 于总体期望。
    \vspace{0.3cm}

    \begin{beamercolorbox}[sep=0.3cm,center,wd=\textwidth]{exampletitle}
        定理 7.2 (切比雪夫弱大数定律)
    \end{beamercolorbox}
    \begin{beamercolorbox}[sep=0.3cm,wd=\textwidth]{examplebody}
        设 $X_1, X_2, \dots, X_n, \dots$ 是一列\alert{相互独立}、具有相同期望 E($X_i$) = $\mu$ 和相同有限方差 D($X_i$) = $\sigma^2 < +\infty$ 的随机变量序列。
        令样本均值为 $\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$。
        则对于任意 $\epsilon > 0$，有：
        \[ \lim_{n \to \infty} P(|\bar{X}_n - \mu| \geq \epsilon) = 0 \]
        或者等价地：
        \[ \lim_{n \to \infty} P(|\bar{X}_n - \mu| < \epsilon) = 1 \]
        这表示样本均值 $\bar{X}_n$ \alert{依概率收敛}于 $\mu$，记作 $\bar{X}_n \xrightarrow{P} \mu$。
    \end{beamercolorbox}
\end{frame}

\begin{frame}[shrink=5]{伯努利弱大数定律}
    \framesubtitle{频率的稳定性}
    \begin{beamercolorbox}[sep=0.3cm,center,wd=\textwidth]{exampletitle}
        定理 7.3 (伯努利弱大数定律)
    \end{beamercolorbox}
    \begin{beamercolorbox}[sep=0.3cm,wd=\textwidth]{examplebody}
        设 $n_A$ 是 $n$ 次独立重复伯努利试验中事件 A 发生的次数，$p$ 是事件 A 在每次试验中发生的概率。
        令 $f_n = \frac{n_A}{n}$ 为事件 A 发生的频率。
        则对于任意 $\epsilon > 0$，有：
        \[ \lim_{n \to \infty} P\left(\left| f_n - p \right| \geq \epsilon\right) = 0 \]
        或者等价地：
        \[ \lim_{n \to \infty} P\left(\left| f_n - p \right| < \epsilon\right) = 1 \]
        伯努利大数定律是切比雪夫弱大数定律的特例，它揭示了“\alert{频率稳定性}”的本质：当试验次数 $n$ 很大时，事件的频率 $f_n$ 会以很高的概率接近其真实的概率 $p$。
    \end{beamercolorbox}
    \vspace{0.3cm}
    \footnotesize
    \textit{注：伯努利大数定律是最早提出的大数定律形式，为用频率估计概率提供了理论依据。}
\end{frame}

\begin{frame}{辛钦弱大数定律 (Khinchin's WLLN)}
    \framesubtitle{更宽松的方差条件}
    辛钦弱大数定律对随机变量序列的方差要求更低。
    \vspace{0.3cm}
    \begin{beamercolorbox}[sep=0.3cm,center,wd=\textwidth]{exampletitle}
        定理 7.4 (辛钦弱大数定律)
    \end{beamercolorbox}
    \begin{beamercolorbox}[sep=0.3cm,wd=\textwidth]{examplebody}
        设 $X_1, X_2, \dots, X_n, \dots$ 是一列\alert{独立同分布 (i.i.d.)} 的随机变量序列，且其共同的期望 E($X_i$) = $\mu$ \alert{存在} (即 $|\mu| < \infty$)。
        令样本均值为 $\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$。
        则对于任意 $\epsilon > 0$，有：
        \[ \lim_{n \to \infty} P(|\bar{X}_n - \mu| \geq \epsilon) = 0 \]
        即 $\bar{X}_n \xrightarrow{P} \mu$。
    \end{beamercolorbox}
    \vspace{0.3cm}
    \textbf{注意：}辛钦WLLN不要求方差存在或有限，仅要求期望存在且序列独立同分布。这使得它比切比雪夫WLLN的适用范围更广。
\end{frame}


\section{强大数定律 (SLLN)}
\begin{frame}{7.3 强大数定律 (Strong Law of Large Numbers, SLLN)}
    \framesubtitle{样本均值的几乎必然收敛}
    强大数定律 (SLLN) 比弱大数定律具有更强的收敛性，它表明样本均值 \alert{几乎必然收敛} (或称以概率1收敛) 于总体期望。
    \vspace{0.3cm}

    \begin{beamercolorbox}[sep=0.3cm,center,wd=\textwidth]{exampletitle}
        定理 7.5 (柯尔莫哥洛夫强大数定律)
    \end{beamercolorbox}
    \begin{beamercolorbox}[sep=0.3cm,wd=\textwidth]{examplebody}
        设 $X_1, X_2, \dots, X_n, \dots$ 是一列\alert{独立同分布 (i.i.d.)} 的随机变量序列，且其共同的期望 E($X_i$) = $\mu$ \alert{存在} (即 $|\mu| < \infty$) 。
        令样本均值为 $\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i$。则：
        \[ P\left( \lim_{n \to \infty} \bar{X}_n = \mu \right) = 1 \]
        这表示样本均值 $\bar{X}_n$ \alert{几乎处处收敛} (Almost Surely Converge) 于 $\mu$，记作 $\bar{X}_n \xrightarrow{a.s.} \mu$。
    \end{beamercolorbox}
    \vspace{0.3cm}
    \footnotesize
    \textit{注：柯尔莫哥洛夫强大数定律的条件与辛钦弱大数定律相同，但结论更强。}
\end{frame}

\begin{frame}[shrink=5]{WLLN 与 SLLN 的区别与联系}
    \framesubtitle{依概率收敛 vs. 几乎必然收敛}
    \textbf{核心区别}：
    \begin{itemize}
        \item \textbf{弱大数定律 (WLLN) - \alert{依概率收敛} ($\xrightarrow{P}$)}:
            \begin{itemize}
                \item $\lim_{n \to \infty} P(|\bar{X}_n - \mu| \geq \epsilon) = 0$
                \item \textbf{含义}：对于任意小的 $\epsilon > 0$ 和 $\delta > 0$，存在一个 $N$，当 $n > N$ 时，$P(|\bar{X}_n - \mu| < \epsilon) > 1 - \delta$。
                \item 描述的是当 $n$ 足够大时，$\bar{X}_n$ \alert{单次观察值}落在 $\mu$ 的小邻域内的\alert{概率}趋近于1。可能存在某些 $n$ 值，$\bar{X}_n$ 偏离 $\mu$ 较远，但这种偏离发生的概率随着 $n$ 增大而减小。
            \end{itemize}
        \item \textbf{强大数定律 (SLLN) - \alert{几乎必然收敛} ($\xrightarrow{a.s.}$)}:
            \begin{itemize}
                \item $P(\lim_{n \to \infty} \bar{X}_n = \mu) = 1$
                \item \textbf{含义}：对于\alert{几乎所有}的样本序列 (即除去一个概率为0的例外集合)，当 $n \to \infty$ 时，$\bar{X}_n$ 的\alert{极限值}就是 $\mu$。
                \item 描述的是\alert{整个序列} $\bar{X}_1, \bar{X}_2, \dots$ 的收敛行为。
            \end{itemize}
    \end{itemize}
    \pause
    \textbf{联系与强度}：
    \begin{itemize}
        \item \alert{SLLN $\implies$ WLLN}：几乎必然收敛是比依概率收敛更强的收敛模式。
        \item \textbf{条件}：SLLN 通常需要与 WLLN 相似或略强的条件 (如柯尔莫哥洛夫 SLLN 和辛钦 WLLN 的条件都是 i.i.d. 和期望存在)。切比雪夫 WLLN 条件不同 (独立、同期望、同有限方差)。
    \end{itemize}
\end{frame}

\section{大数定律的应用}
\begin{frame}[shrink=5]{7.4 大数定律的应用 (Applications of LLN)}
    大数定律是概率论与数理统计之间重要的理论基石，应用广泛：
    \begin{itemize}
        \item \textbf{理论基础与解释现象}：
            \begin{itemize}
                \item \alert{频率的稳定性}：为用频率近似概率提供了坚实的理论依据 (伯努利大数定律)。
                \item 解释了在大量重复试验中，随机事件的平均结果会趋于一个稳定的值。
            \end{itemize}
        \item \textbf{统计推断}：
            \begin{itemize}
                \item \alert{参数估计}：样本均值是总体期望的\alert{一致估计量}。这是矩估计法等参数估计方法的基础。
                \item \alert{蒙特卡洛方法}：通过大量随机抽样来模拟复杂过程，估计积分、期望等数值解。例如，用投点法计算 $\pi$ 的值。
            \end{itemize}
        \item \textbf{风险管理与保险}：
            \begin{itemize}
                \item 保险公司利用大数定律来预测在大量保单中可能发生的赔付总额，从而制定合理的保费，确保经营稳定。
            \end{itemize}
        \item \textbf{质量控制与抽样检验}：
            \begin{itemize}
                \item 通过对产品进行抽样检查，根据样本的合格率来推断整批产品的质量水平。
            \end{itemize}
        \item \textbf{物理与工程}：
            \begin{itemize}
                \item 测量误差的分析：多次测量的平均值通常比单次测量更接近真实值。
            \end{itemize}
    \end{itemize}
\end{frame}

\section{总结与展望}
\begin{frame}[shrink=5]{总结与展望}
    \begin{block}{本讲小结}
        \begin{itemize}
            \item \textbf{切比雪夫不等式}：提供了概率的通用上界。
            \item \textbf{弱大数定律 (WLLN)}：样本均值依概率收敛于总体期望。
                \begin{itemize}
                    \item 切比雪夫WLLN (独立, 同期望, 同有限方差)
                    \item 伯努利WLLN (频率稳定性)
                    \item 辛钦WLLN (i.i.d., 期望存在)
                \end{itemize}
            \item \textbf{强大数定律 (SLLN)}：样本均值几乎必然收敛于总体期望 (i.i.d., 期望存在)。
            \item SLLN 比 WLLN \alert{更强}，提供了关于整个样本路径收敛性的保证。
            \item 大数定律是连接概率论与统计实践的\alert{核心桥梁}。
        \end{itemize}
    \end{block}
    \pause
    \begin{alertblock}{展望}
        \begin{itemize}
            \item 大数定律有多种形式，对应不同的条件和收敛强度。
            \item 除了大数定律，概率论中的另一个核心极限定理是\alert{中心极限定理 (Central Limit Theorem, CLT)}，它描述了样本均值的分布形态，将在后续课程中讨论。
            \item 大数定律和中心极限定理共同构成了现代统计推断的理论基石。
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}
    \centering
    \Huge{\bfseries 谢谢聆听！}
    \vspace{1cm}
    \normalsize
    问题与讨论
\end{frame}

\end{document}